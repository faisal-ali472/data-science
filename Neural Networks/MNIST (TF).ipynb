{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression & Neural Networks\n",
    "\n",
    "Dataset: MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change the temp directory from user/app data ... to E: drive to avoid IO related issues.\n",
    "from urllib import request\n",
    "request.tempfile.tempdir = 'E:/tensorflow/temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting E:\\tensorflow\\train-images-idx3-ubyte.gz\n",
      "Extracting E:\\tensorflow\\train-labels-idx1-ubyte.gz\n",
      "Extracting E:\\tensorflow\\t10k-images-idx3-ubyte.gz\n",
      "Extracting E:\\tensorflow\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data, mnist_softmax\n",
    "mnist = input_data.read_data_sets(\"E:\\\\tensorflow\", one_hot=True)\n",
    "#data_sets = input_data.read_data_sets(FLAGS.train_dir, FLAGS.fake_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# define placeholder for input\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# define weights and bias\n",
    "W = tf.Variable(tf.zeros([784,10]),dtype=tf.float32)\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# define regression model y = Wx + b; apply softmax for classification\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# define placeholder for labels\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# define cross-entry loss function\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "# define optimizer to minimize loss\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "# initialize variables\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training\n",
    "    \n",
    "    - Using loss function called \"cross-entropy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training model for 1000 steps with batch of 100\n",
    "for _ in range(1000):\n",
    "    # get next batch\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    # train model using current input batch\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# define variables\n",
    "num_input = 784\n",
    "num_classes = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "n_hidden_1 = 400\n",
    "n_hidden_2 = 100\n",
    "\n",
    "# define placeholder for input and labels\n",
    "x = tf.placeholder(tf.float32, [None, num_input])\n",
    "y_ = tf.placeholder(tf.float32, [None, num_classes])\n",
    "\n",
    "# layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "# construct model [784 -> 500 -> 200 -> 10]\n",
    "layer0 = x\n",
    "layer1 = tf.nn.sigmoid(tf.matmul(layer0, weights['h1']) + biases['b1'])   # here using sigmoid increases accuracy greatly from no activation or using relu\n",
    "layer2 = tf.nn.sigmoid(tf.matmul(layer1, weights['h2']) + biases['b2'])\n",
    "output_layer = tf.matmul(layer2, weights['out']) + biases['out']\n",
    "\n",
    "# define cross entropy loss function\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output_layer, labels=y_))\n",
    "\n",
    "# define adam as optimizer\n",
    " \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# minimize on each training step\n",
    "train = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(output_layer, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Minibatch Loss= 5.4860, Training Accuracy= 0.110\n",
      "Step 1, Minibatch Loss= 4.2184, Training Accuracy= 0.190\n",
      "Step 100, Minibatch Loss= 0.4732, Training Accuracy= 0.870\n",
      "Step 200, Minibatch Loss= 0.4730, Training Accuracy= 0.850\n",
      "Step 300, Minibatch Loss= 0.3066, Training Accuracy= 0.900\n",
      "Step 400, Minibatch Loss= 0.2163, Training Accuracy= 0.940\n",
      "Step 500, Minibatch Loss= 0.1975, Training Accuracy= 0.950\n",
      "Step 600, Minibatch Loss= 0.1794, Training Accuracy= 0.950\n",
      "Step 700, Minibatch Loss= 0.0954, Training Accuracy= 0.970\n",
      "Step 800, Minibatch Loss= 0.2783, Training Accuracy= 0.950\n",
      "Step 900, Minibatch Loss= 0.1041, Training Accuracy= 0.970\n",
      "Step 1000, Minibatch Loss= 0.3376, Training Accuracy= 0.910\n",
      "Step 1100, Minibatch Loss= 0.1438, Training Accuracy= 0.950\n",
      "Step 1200, Minibatch Loss= 0.0451, Training Accuracy= 1.000\n",
      "Step 1300, Minibatch Loss= 0.2090, Training Accuracy= 0.960\n",
      "Step 1400, Minibatch Loss= 0.0984, Training Accuracy= 0.960\n",
      "Step 1500, Minibatch Loss= 0.1719, Training Accuracy= 0.960\n",
      "Step 1600, Minibatch Loss= 0.1010, Training Accuracy= 0.960\n",
      "Step 1700, Minibatch Loss= 0.0654, Training Accuracy= 0.990\n",
      "Step 1800, Minibatch Loss= 0.1300, Training Accuracy= 0.940\n",
      "Step 1900, Minibatch Loss= 0.1496, Training Accuracy= 0.970\n",
      "Step 2000, Minibatch Loss= 0.0494, Training Accuracy= 0.980\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.946\n"
     ]
    }
   ],
   "source": [
    "# initialize variables\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# train model\n",
    "for step in range(2001):\n",
    "    # get training batch\n",
    "    batch_x, batch_y = mnist.train.next_batch(100)\n",
    "    # train model on training batch\n",
    "    sess.run(train,feed_dict={x:batch_x, y_:batch_y})\n",
    "    \n",
    "    if step % 100 == 0 or step == 1:\n",
    "        loss, acc = sess.run([loss_op, accuracy], feed_dict={x: batch_x, y_: batch_y})\n",
    "        print(\"Step \" + str(step) + \", Minibatch Loss= \" + \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \"{:.3f}\".format(acc))\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Calculate accuracy for MNIST test images\n",
    "print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
